#below taken from
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
seeds <- vector(mode = "list", length = 11)
for(i in 1:10) seeds[[i]]<- sample.int(n=1000, 57)
seeds[[11]] <- sample.int(1000, 1)
fitControl <- trainControl(method = "cv",
number = 10,
allowParallel = TRUE, seeds=seeds)
modelFit <- train(classe ~ .,method="rf",data=tidytrain,trControl = fitControl)
stopCluster(cluster)
registerDoSEQ()
print(modelFit)
modelFit$resample
confusionMatrix.train(modelFit)
```
## Discussion
The above results from the model show a very good accuracy and kappa for all 10 fold resamples. This means that the estimated out of sample error is very small so the model appears to be performing well.
dim(tidytrain)
dim(train)
dim(test)
head(test)
prediction <- predict(modelFit,newdata=test)
prediction
dim(test)
---
title: "How accelerometers, gyroscopes and magnetometers data can tell us whether people exercise correctly"
author: "Nathalie Descusse-Brown"
date: "April 03, 2019"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=FALSE,message=FALSE)
```
## Executive Summary
This paper makes use of data from accelerometers, gyroscopes and magnetometers on the belt, forearm, arm, and dumbell of 6 participants. These participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The aim of this paper is to use the data to predict the manner in which the participants  did the exercise. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).
## Data Processing
The training dataset was accessed at the following site and assigned to the train tibble: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
The testing dataset was accessed at the following site and assigned to the test tibble:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv
```{r dataprocessing, cache=TRUE}
## load all data necessary to the assignment
library(tidyverse)
if (!file.exists("pml-training.csv")) {
fileUrl1 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
download.file(fileUrl1,destfile="pml-training.csv")
fileUrl2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(fileUrl2,destfile="pml-testing.csv")
}
train <- read_csv("pml-training.csv")
test <- read_csv("pml-testing.csv")
```
## Exploratory Analysis
We know from the data source that the data is classified as follows:
- Class A: performed exactly according to the specification
- Class B: throwing the elbows to the front
- Class C: lifting the dumbbell only halfway
- Class D: lowering the dumbbell only halfway
- Class E: throwing the hips to the front.
As there are 160 columns, the tibble train was first tidied up by removing the columns with a significant number of NA, empty, or #DIV/0! values. This leaves only 60 columns, which will significantly improve processing time.
```{r exploratoryanalysis}
library(plyr)
library(Hmisc)
tidytrain <- data.frame(stringsAsFactors=FALSE)
testtrain <- data.frame(stringsAsFactors=FALSE)
j <- 1
for (i in 1:dim(train)[2])
{
if (((count(is.na(train[,i]))[[1,2]]>11773) & (count(train[,i]=="")[[1,2]]>7849) & (count(train[,i]=="#DIV/0!")[[1,2]]>7849)) & length(tidytrain)==0)
{
tidytrain <- train[,i]
testtrain <- test[,i]
# firstname <- colnames(train)[i]
j <- j+1
}
else if (((count(is.na(train[,i]))[[1,2]]>11773) & (count(train[,i]=="")[[1,2]]>7849) & (count(train[,i]=="#DIV/0!")[[1,2]]>7849)) & length(tidytrain)>0)
{
tidytrain <- cbind(tidytrain,train[,i])
testtrain <- cbind(testtrain,test[,i])
colnames(tidytrain)[j] <- colnames(train)[i]
colnames(testtrain)[j] <- colnames(test)[i]
j <- j+1
}
}
```
Because of the classification and the location of the measuring devices we make the following assumptions:
- Class B should be associated with a negative gyros_dumbbell_z as the dumbell is pushed towards the shoulder when the elbow goes to the front.
- Class C should be associated with smaller gyros_dumbbell_y and gyros_forearm_y compared to Class A and Class B.
- Class D should be associated with similar amplitude gyros_dumbbell_y and gyros_forearm_y compared to Class C but shifted along the y axis.
- Class E should be associated with larger gyros_belt_z compared to other classes.
```{r ggpairexploratory, out.width = '1000px', dpi=300}
library(gridExtra)
p = ggplot(tidytrain,aes(y=accel_dumbbell_z,x=accel_dumbbell_x,colour=classe))+geom_point(colour="grey50",size=0.5)+geom_point(size=0.5) + ylim(-2,2)
p2 = ggplot(tidytrain,aes(y=accel_dumbbell_y,x=accel_forearm_y,colour=classe))+geom_point(colour="pink",size=0.5)+
geom_point(size=0.5) + ylim(-2,2)
p3 = ggplot(tidytrain,aes(y=accel_belt_z,x=accel_forearm_z,colour=classe))+geom_point(colour="pink",size=0.5)+
geom_point(size=0.5) + ylim(-2,2)
grid.arrange(p,p2,p3,nrow=2, ncol=2)
```
We can see from the above that our assumptions were not verified as there is significant overlap (noise) between the data for the different classes. Owing to this and the fact that only a few of the covariates were considered in the exploratory analysis due to extend of the work required to look at all covariates, it was deemed more efficient to process with model selection.
## Model Selection and Results
Because no specific clusters were identified in the exploratory analysis, a random forest model was selected as it is known to be widely used and highly accurate without making any assumption as to the distribution of the data. However, speed was an issue when running the model at first. Some research into parallel processing led us to this very helpful guidance: https://github.com/lgreski/datasciencectacontent/blob/master/markdown/pml-randomForestPerformance.md. The computational speed was significantly improved by implementing parallel processing. The seeds argument of the trainControl function was also used to ensure reproducibility. Cross-validation was used as the training sample is relatively small with only 6 individuals.
```{r prediction}
library(caret)
library(rattle)
library(MASS)
library(parallel)
library(doParallel)
#below taken from
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
seeds <- vector(mode = "list", length = 11)
for(i in 1:10) seeds[[i]]<- sample.int(n=1000, 57)
seeds[[11]] <- sample.int(1000, 1)
fitControl <- trainControl(method = "cv",
number = 10,
allowParallel = TRUE, seeds=seeds)
modelFit <- train(classe ~ .,method="rf",data=tidytrain,trControl = fitControl)
stopCluster(cluster)
registerDoSEQ()
print(modelFit)
modelFit$resample
confusionMatrix.train(modelFit)
```
## Discussion
The above results from the model show a very good accuracy and kappa for all 10 fold resamples. This means that the estimated out of sample error is very small so the model appears to be performing well.
## Test
```{r predicting}
prediction <- predict(modelFit,newdata=test)
prediction
```
prediction2 <- predict(modelFit,newdata=tidytest)
prediction2 <- predict(modelFit,newdata=testtrain)
prediction <- predict(modelFit,newdata=test)
prediction2
prediction <- predict(modelFit,newdata=test[1,])
prediction <- predict(modelFit,newdata=test)
prediction
prediction <- predict(modelFit,newdata=test[1,])
prediction
prediction <- predict(modelFit,newdata=test[2,])
prediction
test[2,]
prediction <- predict(modelFit,data=tidytrain)
prediction
?predict
prediction <- predict(modelFit,test[2,])
prediction <- predict(modelFit,data=tidytrain)
prediction
prediction <- predict(modelFit,data=train)
prediction <- predict(modelFit,newdata=test[2,])
prediction <- predict(modelFit,data=train)
prediction <- predict(modelFit,data=tidytrain)
prediction
prediction <- predict(modelFit,data=tidytrain
)
prediction <- predict.train(modelFit,data=tidytrain)
modelFit <- train(classe ~ .,method="rf",data=tidytrain,trControl = fitControl)
head(tidytrain)
tidytrain[,60]
dim(tidytrain)
tidytrain[19620:19622,60]
testtrain[,60]
names(testtrain)
names(tidytrain)
head(test)
names(test)
prediction <- predict.train(modelFit,data=testtrain[,-60])
prediction <- predict(modelFit,newdata=testtrain[,-60])
prediction
prediction2 <- predict(modelFit,tidytrain[,-60])
prediction2
tail(prediction2)
prediction <- predict(modelFit,newdata=testtrain[,-60])
prediction
names(tests)
names(test)
names(tidytrain)
---
title: "How accelerometers, gyroscopes and magnetometers data can tell us whether people exercise correctly"
author: "Nathalie Descusse-Brown"
date: "April 03, 2019"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=FALSE,message=FALSE)
```
## Executive Summary
This paper makes use of data from accelerometers, gyroscopes and magnetometers on the belt, forearm, arm, and dumbell of 6 participants. These participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The aim of this paper is to use the data to predict the manner in which the participants  did the exercise. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).
## Data Processing
The training dataset was accessed at the following site and assigned to the train tibble: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
The testing dataset was accessed at the following site and assigned to the test tibble:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv
```{r dataprocessing, cache=TRUE}
## load all data necessary to the assignment
library(tidyverse)
if (!file.exists("pml-training.csv")) {
fileUrl1 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
download.file(fileUrl1,destfile="pml-training.csv")
fileUrl2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(fileUrl2,destfile="pml-testing.csv")
}
train <- read_csv("pml-training.csv")
test <- read_csv("pml-testing.csv")
```
## Exploratory Analysis
We know from the data source that the data is classified as follows:
- Class A: performed exactly according to the specification
- Class B: throwing the elbows to the front
- Class C: lifting the dumbbell only halfway
- Class D: lowering the dumbbell only halfway
- Class E: throwing the hips to the front.
As there are 160 columns, the tibble train was first tidied up by removing the columns with a significant number of NA, empty, or #DIV/0! values. This leaves only 60 columns, which will significantly improve processing time.
```{r exploratoryanalysis}
library(plyr)
library(Hmisc)
tidytrain <- data.frame(stringsAsFactors=FALSE)
tidytest <- data.frame(stringsAsFactors=FALSE)
j <- 1
for (i in 1:dim(train)[2])
{
if (((count(is.na(train[,i]))[[1,2]]>11773) & (count(train[,i]=="")[[1,2]]>7849) & (count(train[,i]=="#DIV/0!")[[1,2]]>7849)) & length(tidytrain)==0)
{
tidytrain <- train[,i]
tidytest <- test[,i]
# firstname <- colnames(train)[i]
j <- j+1
}
else if (((count(is.na(train[,i]))[[1,2]]>11773) & (count(train[,i]=="")[[1,2]]>7849) & (count(train[,i]=="#DIV/0!")[[1,2]]>7849)) & length(tidytrain)>0)
{
tidytrain <- cbind(tidytrain,train[,i])
tidytest <- cbind(tidytest,test[,i])
colnames(tidytrain)[j] <- colnames(train)[i]
colnames(tidytest)[j] <- colnames(test)[i]
j <- j+1
}
}
tidytrain <- tidytrain[,-c(1,2)]
tidytest <- tidytest[,-c(1,2)]
```
Because of the classification and the location of the measuring devices we make the following assumptions:
- Class B should be associated with a negative gyros_dumbbell_z as the dumbell is pushed towards the shoulder when the elbow goes to the front.
- Class C should be associated with smaller gyros_dumbbell_y and gyros_forearm_y compared to Class A and Class B.
- Class D should be associated with similar amplitude gyros_dumbbell_y and gyros_forearm_y compared to Class C but shifted along the y axis.
- Class E should be associated with larger gyros_belt_z compared to other classes.
```{r ggpairexploratory, out.width = '1000px', dpi=300}
library(gridExtra)
p = ggplot(tidytrain,aes(y=accel_dumbbell_z,x=accel_dumbbell_x,colour=classe))+geom_point(colour="grey50",size=0.5)+geom_point(size=0.5) + ylim(-2,2)
p2 = ggplot(tidytrain,aes(y=accel_dumbbell_y,x=accel_forearm_y,colour=classe))+geom_point(colour="pink",size=0.5)+
geom_point(size=0.5) + ylim(-2,2)
p3 = ggplot(tidytrain,aes(y=accel_belt_z,x=accel_forearm_z,colour=classe))+geom_point(colour="pink",size=0.5)+
geom_point(size=0.5) + ylim(-2,2)
grid.arrange(p,p2,p3,nrow=2, ncol=2)
```
We can see from the above that our assumptions were not verified as there is significant overlap (noise) between the data for the different classes. Owing to this and the fact that only a few of the covariates were considered in the exploratory analysis due to extend of the work required to look at all covariates, it was deemed more efficient to process with model selection.
## Model Selection and Results
Because no specific clusters were identified in the exploratory analysis, a random forest model was selected as it is known to be widely used and highly accurate without making any assumption as to the distribution of the data. However, speed was an issue when running the model at first. Some research into parallel processing led us to this very helpful guidance: https://github.com/lgreski/datasciencectacontent/blob/master/markdown/pml-randomForestPerformance.md. The computational speed was significantly improved by implementing parallel processing. The seeds argument of the trainControl function was also used to ensure reproducibility. Cross-validation was used as the training sample is relatively small with only 6 individuals.
```{r prediction}
library(caret)
library(rattle)
library(MASS)
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
# seeds <- vector(mode = "list", length = 11)
# for(i in 1:10) seeds[[i]]<- sample.int(n=1000, 57)
# seeds[[11]] <- sample.int(1000, 1)
seeds <- vector(mode = "list", length = 6)
for(i in 1:5) seeds[[i]]<- sample.int(n=1000, 57)
seeds[[6]] <- sample.int(1000, 1)
fitControl <- trainControl(method = "cv",
number = 5,
allowParallel = TRUE, seeds=seeds)
# fitControl <- trainControl(allowParallel = TRUE, seeds)
modelFit <- train(classe ~ .,method="rf",data=tidytrain,trControl = fitControl)
stopCluster(cluster)
registerDoSEQ()
print(modelFit)
modelFit$resample
confusionMatrix.train(modelFit)
```
## Discussion
The above results from the model show a very good accuracy and kappa for all 10 fold resamples. This means that the estimated out of sample error is very small so the model appears to be performing well.
## Test
```{r predicting}
prediction <- predict(modelFit,newdata=tidytest[,-c(1,60)])
prediction
```
names(tidytrain)
names(tidytest)
prediction <- predict(modelFit,newdata=tidytest[,-60])
prediction
install.packages("miniUI")
library(shiny)
library(miniUI)
pickTrees <- function() {
ui <- miniPage(
gadgetTitleBar("Select Points by Dragging your Mouse"),
miniContentPanel(
plotOutput("plot", height = "100%", brush = "brush")
)
)
server <- function(input, output, session) {
output$plot <- renderPlot({
plot(trees$Girth, trees$Volume, main = "Trees!",
xlab = "Girth", ylab = "Volume")
})
observeEvent(input$done, {
stopApp(brushedPoints(trees, input$brush,
xvar = "Girth", yvar = "Volume"))
})
}
runGadget(ui, server)
}
pickTrees()
test = pickTrees()
test
library(shiny)
shinyUI(fluidPage(
titlePanel("Tabs!"),
sidebarLayout(
sidebarPanel(
textInput("box1", "Enter Tab 1 Text:", value = "Tab 1!"),
textInput("box2", "Enter Tab 2 Text:", value = "Tab 2!"),
textInput("box3", "Enter Tab 3 Text:", value = "Tab 3!")
),
mainPanel(
tabsetPanel(type = "tabs",
tabPanel("Tab 1", br(), textOutput("out1")),
tabPanel("Tab 2", br(), textOutput("out2")),
tabPanel("Tab 2", br(), textOutput("out3"))
)
)
)
))
library(shiny)
shinyServer(function(input, output) {
output$out1 <- renderText(input$box1)
output$out2 <- renderText(input$box2)
output$out3 <- renderText(input$box3)
})
library(webshot)
appshot("app3", "app3.png")
library(shiny)
shinyUI(fluidPage(
titlePanel("Tabs!"),
sidebarLayout(
sidebarPanel(
textInput("box1", "Enter Tab 1 Text:", value = "Tab 1!"),
textInput("box2", "Enter Tab 2 Text:", value = "Tab 2!"),
textInput("box3", "Enter Tab 3 Text:", value = "Tab 3!")
),
mainPanel(
tabsetPanel(type = "tabs",
tabPanel("Tab 1", br(), textOutput("out1")),
tabPanel("Tab 2", br(), textOutput("out2")),
tabPanel("Tab 2", br(), textOutput("out3"))
)
)
)
))
library(shiny)
shinyServer(function(input, output) {
output$out1 <- renderText(input$box1)
output$out2 <- renderText(input$box2)
output$out3 <- renderText(input$box3)
})
install.packages("webshot")
library(webshot)
appshot("app3", "app3.png")
library(webshot)
appshot("app3", "app3.png")
library(DDPQuiz3
)
install.packages("C:/Users/natha/Developing_Data_Products/DDPQuiz3_1.0.zip", repos = NULL, type = "win.binary")
install.packages("C:/Users/natha/Developing_Data_Products/DDPQuiz3_1.0.zip", repos = NULL, type = "win.binary")
library(installr)
install.packages("C:/Users/natha/Developing_Data_Products/DDPQuiz3_1.0.zip", repos = NULL, type = "win.binary")
library(DDPQuiz3)
install.packages("C:/Users/natha/Developing_Data_Products/DDPQuiz3_1.0.zip", repos = NULL, type = "win.binary")
library(DDPQuiz3)
.libPaths()
?install.packages
install.packages("C:/Users/natha/Developing_Data_Products/DDPQuiz3_1.0.zip","C:/Program Files/R/R-3.5.1/library", repos = NULL, type = "win.binary")
library(DDPQuiz3)
install.packages("C:/Users/natha/Developing_Data_Products/DDPQuiz3_1.0.zip","C:/Program Files/R/R-3.5.1/library")
install.packages("C:/Users/natha/Developing_Data_Products/DDPQuiz3_1.0.zip")
shiny::runApp('C:/Users/natha/Developing_Data_Products/ParkrunFinder')
runApp('C:/Users/natha/Developing_Data_Products/ParkrunFinder')
runApp('C:/Users/natha/Developing_Data_Products/ParkrunFinder')
source('C:/Users/natha/Developing_Data_Products/ParkrunFinder/findParkrunUK.R')
runApp('C:/Users/natha/Developing_Data_Products/ParkrunFinder')
source('C:/Users/natha/Developing_Data_Products/ParkrunFinder/findParkrunUK.R')
PR[i] <- webpage %>%
html_nodes(xpath = paste0('//*[@id="mw-content-text"]/div/table[', i, ']')) %>%
html_table(fill = TRUE)
names(PR[[i]])<- names(PR[[1]])
if (i==1) {PRdf <- PR[[1]]
PRdf$Region <- PRregions[1]}
else {PRdfinterm <- cbind(PR[[i]],Region=PRregions[i])
PRdf <- rbind(PRdf,PRdfinterm)  }
# PRdf$region <- PRregions[i]
}
baseurl <- "https://en.wikipedia.org/wiki/"
lat <- vector(mode="character")
long <- vector(mode="character")
if (file.exists("latlong.Rdata")) {
load("latlong.Rdata")
}
else {
for (j in 1:dim(PRdf)[1]) {
townurl <- paste0(baseurl, gsub("^(.*?),.*", "\\1", PRdf[j,2]))
townurl <- gsub(" ", "_", townurl)
if (url.exists(townurl)) {
lattemp <- read_html(curl(townurl),handle = curl::new_handle("useragent" = "Mozilla/5.0")) %>%
html_nodes(".latitude")%>%
html_text(trim = TRUE) %>%
strsplit(split = "\n")
longtemp <- read_html(curl(townurl),handle = curl::new_handle("useragent" = "Mozilla/5.0")) %>%
html_nodes(".longitude")%>%
html_text(trim = TRUE) %>%
strsplit(split = "\n")
}
if (length(lattemp)>0 & url.exists(townurl)) {lat[j] <- as.character(lattemp)} else {lat[j] <- ""}
if (length(longtemp)>0 & url.exists(townurl)) {long[j] <- as.character(longtemp)} else {long[j] <- ""}
}
save(lat,long,file="latlong.Rdata")
}
PRdf$Region <- substring(PRdf$Region,regexpr("[A-Z]",PRdf$Region))
PRdf$Lat <- lat
PRdf$Long <- long
PRdf$Name <- gsub("[[0-9](.*?)]","",as.character(PRdf$Name))
PRdf
findParkrunUK <- function() {
library(rvest)
library(curl)
library(RCurl)
url <- "https://en.wikipedia.org/wiki/List_of_Parkruns_in_the_United_Kingdom"
PRregions <- url %>%
read_html() %>%
html_nodes("ul") %>%
html_text(trim = TRUE) %>%
strsplit(split = "\n") %>%
unlist()
PRdf <- data.frame()
PR <- list()
for (i in 1:13) {
webpage <- read_html(url)
PR[i] <- webpage %>%
html_nodes(xpath = paste0('//*[@id="mw-content-text"]/div/table[', i, ']')) %>%
html_table(fill = TRUE)
names(PR[[i]])<- names(PR[[1]])
if (i==1) {PRdf <- PR[[1]]
PRdf$Region <- PRregions[1]}
else {PRdfinterm <- cbind(PR[[i]],Region=PRregions[i])
PRdf <- rbind(PRdf,PRdfinterm)  }
# PRdf$region <- PRregions[i]
}
baseurl <- "https://en.wikipedia.org/wiki/"
lat <- vector(mode="character")
long <- vector(mode="character")
if (file.exists("latlong.Rdata")) {
load("latlong.Rdata")
}
else {
for (j in 1:dim(PRdf)[1]) {
townurl <- paste0(baseurl, gsub("^(.*?),.*", "\\1", PRdf[j,2]))
townurl <- gsub(" ", "_", townurl)
if (url.exists(townurl)) {
lattemp <- read_html(curl(townurl),handle = curl::new_handle("useragent" = "Mozilla/5.0")) %>%
html_nodes(".latitude")%>%
html_text(trim = TRUE) %>%
strsplit(split = "\n")
longtemp <- read_html(curl(townurl),handle = curl::new_handle("useragent" = "Mozilla/5.0")) %>%
html_nodes(".longitude")%>%
html_text(trim = TRUE) %>%
strsplit(split = "\n")
}
if (length(lattemp)>0 & url.exists(townurl)) {lat[j] <- as.character(lattemp)} else {lat[j] <- ""}
if (length(longtemp)>0 & url.exists(townurl)) {long[j] <- as.character(longtemp)} else {long[j] <- ""}
}
save(lat,long,file="latlong.Rdata")
}
PRdf$Region <- substring(PRdf$Region,regexpr("[A-Z]",PRdf$Region))
PRdf$Lat <- lat
PRdf$Long <- long
PRdf$Name <- gsub("[[0-9](.*?)]","",as.character(PRdf$Name))
PRdf
}
rm(list=ls())
system("cmd.exe /C dir")
system("cmd.exe /C dir /AH")
source('C:/Users/natha/OneDrive/Desktop/Monmouth/monmouthtreatment.R')
source('C:/Users/natha/OneDrive/Desktop/Monmouth/monmouthtreatment.R')
source('C:/Users/natha/OneDrive/Desktop/Monmouth/monmouthtreatment.R')
source('C:/Users/natha/OneDrive/Desktop/Monmouth/monmouthtreatment.R')
# Chunk 1: plot1
source('C:/Users/natha/Desktop/Monmouth/monmouthgeneral.R')
monmouthgeneral("plot1")
# Chunk 2: plot2
source('C:/Users/natha/Desktop/Monmouth/monmouthgeneral.R')
monmouthgeneral("plot2")
# Chunk 3: table1
source('C:/Users/natha/Desktop/Monmouth/monmouthtreatment.R')
results1 <- monmouthtreatment("totalincrease")
results1
# Chunk 4: table2
source('C:/Users/natha/Desktop/Monmouth/monmouthtreatment.R')
results2 <- monmouthtreatment("patientcostincrease")
results2
shiny::runApp('C:/Users/natha/CapstonePredictiveTexting/Coursera-SwiftKey/final/MindReader')
